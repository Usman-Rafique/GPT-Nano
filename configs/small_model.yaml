# model
model_name: 'nanoGPT_small'
block_size: 8  # maximum context size
embedding_n: 32
num_attention_heads: 4
num_layers: 3  # number of Blocks in the transformer

# optimization
lr: 3e-4
batch_size: 64
dropout: 0.0
num_train_iterations: 5000

compile_model: True # requires PyTorch 2, but it is much faster

# training dataset
input_fname: 'data/shakespeare.txt' # other option: 'data/sherlock_holmes.txt'